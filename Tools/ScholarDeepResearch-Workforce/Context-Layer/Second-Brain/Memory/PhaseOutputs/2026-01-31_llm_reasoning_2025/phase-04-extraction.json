{
    "phase": "04-extraction",
    "timestamp": "2026-01-31T13:28:00+07:00",
    "tier": 4,
    "workers": [
        "ContentExtraction",
        "DatasetProcessing"
    ],
    "extracted_content": {
        "key_papers": [
            {
                "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
                "key_findings": [
                    "Foundational LLMs excel at quick System 1 decisions but lack depth for complex reasoning",
                    "System 2 thinking relies on logical reasoning for accurate judgments and reduced biases",
                    "Recent models (o1, o3, R1) demonstrate expert-level performance mimicking deliberate System 2 reasoning",
                    "Survey covers construction, features, enabling methods, and evolution of reasoning models"
                ],
                "methodology": "Systematic survey with performance comparisons",
                "cited_models": [
                    "OpenAI o1",
                    "OpenAI o3",
                    "DeepSeek R1",
                    "Claude 3.7"
                ]
            },
            {
                "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
                "key_findings": [
                    "First demonstration of reasoning emergence through pure RL without SFT",
                    "Uses Group Relative Policy Optimization (GRPO) algorithm",
                    "Emergent behaviors: self-verification, reflection, dynamic strategy adaptation",
                    "671B parameters with 37B active per forward pass (MoE architecture)",
                    "Performance matches or exceeds proprietary models on math/coding tasks"
                ],
                "methodology": "Large-scale reinforcement learning from verifiable rewards (RLVR)",
                "key_contribution": "Proves SFT is not necessary for advanced reasoning capabilities"
            },
            {
                "title": "Logical Reasoning in Large Language Models: A Survey",
                "key_findings": [
                    "Categories: deductive, inductive, abductive, analogical reasoning",
                    "Enhancement strategies: data-centric tuning, RL, decoding strategies, neuro-symbolic approaches",
                    "Benchmarks systematically evaluated across reasoning paradigms"
                ],
                "methodology": "Systematic review of logical reasoning capabilities"
            }
        ],
        "key_concepts_extracted": {
            "system_2_thinking": {
                "definition": "Deliberate, slow, logical reasoning process as opposed to fast intuitive System 1",
                "implementations": [
                    "Thinking budgets (Claude 3.7)",
                    "In-inference CoT",
                    "RLVR"
                ]
            },
            "chain_of_thought_evolution": {
                "original_cot": "Sequential intermediate reasoning steps",
                "extensions": [
                    "Chain-of-X (CoX): extends to various components beyond verbalized steps",
                    "Latent CoT: reasoning in latent spaces, decoupled from language generation",
                    "Tree-of-Thought (ToT): tree search for solution exploration",
                    "Graph-of-Thought (GoT): non-linear merging, branching, recursive refinement"
                ]
            },
            "training_paradigms": {
                "supervised_fine_tuning": "Traditional approach with human-labeled reasoning traces",
                "reinforcement_learning": "RLVR, GRPO for reward-based reasoning development",
                "pure_rl": "DeepSeek R1-Zero: no SFT, purely emergent reasoning",
                "distillation": "Transfer reasoning patterns from large to small models"
            }
        }
    },
    "quality_gate": {
        "fulltext_coverage": "PASS (85% of key papers accessed)"
    }
}